"""
arca.live ëŒ“ê¸€ í¬ë¡¤ëŸ¬
"""
import requests
from bs4 import BeautifulSoup
import time
from typing import List, Dict
import re

class ArcaLiveCrawler:
    # HTTP ìš”ì²­ ì‹œ ì‚¬ìš©í•  í—¤ë” (ìµœì‹  Chrome User-Agent ì‚¬ìš©)
    HEADERS = {
        # ìµœì‹  Chrome User-Agent (2025ë…„ ê¸°ì¤€)
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',
        # ìš”ì²­ì´ ì•„ì¹´ë¼ì´ë¸Œ ë‚´ë¶€ì—ì„œ ì˜¨ ê²ƒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” Referer ìœ ì§€
        'Referer': 'https://arca.live/',
    }

    def __init__(self):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ë° requests ì„¸ì…˜ ìƒì„±.
        """
        print("Crawler initialized (using requests/BeautifulSoup Session).")
        self.post_author = ""
        # requests.Sessionì„ ì‚¬ìš©í•˜ì—¬ ì—°ê²°ì„ ìœ ì§€í•˜ê³  ì¿ í‚¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
        self.session = requests.Session() 

    def _extract_post_author(self, soup: BeautifulSoup) -> str:
        """ê²Œì‹œê¸€ ì‘ì„±ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        member_info = soup.select_one('.member-info .user-info a')
        post_author = ""
        if member_info:
            post_author = member_info.get('data-filter', '').strip()
            if not post_author:
                post_author = member_info.get_text(strip=True)
        return post_author

    def get_comments(self, url: str) -> List[Dict[str, str]]:
        """
        ì§€ì •ëœ URLì—ì„œ ëŒ“ê¸€ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ.
        
        403 ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ ê²Œì‹œê¸€ URL ìš”ì²­ ì „, ë©”ì¸ ë„ë©”ì¸ì— ìš”ì²­í•˜ì—¬
        í•„ìˆ˜ ì¿ í‚¤(ì˜ˆ: ë³´ì•ˆ ì¿ í‚¤)ë¥¼ ì„¸ì…˜ì— ë¯¸ë¦¬ í™•ë³´í•©ë‹ˆë‹¤.
        """
        
        # 0. í•„ìˆ˜ ì¿ í‚¤ í™•ë³´ ë‹¨ê³„ (403 ìš°íšŒ ì‹œë„)
        try:
            base_url_match = re.match(r'(https?://[^/]+)', url)
            if base_url_match:
                base_url = base_url_match.group(0)
                # ë©”ì¸ í˜ì´ì§€ì— ë¨¼ì € ì ‘ê·¼í•˜ì—¬ í•„ìš”í•œ ì¿ í‚¤ë¥¼ ì„¸ì…˜ì— ì €ì¥
                print(f"âœ¨ ì´ˆê¸° ì¿ í‚¤ í™•ë³´ë¥¼ ìœ„í•´ ë©”ì¸ í˜ì´ì§€ ({base_url})ì— ì ‘ê·¼í•©ë‹ˆë‹¤.")
                self.session.get(base_url, headers=self.HEADERS, timeout=5)
                time.sleep(0.5) # ì§§ì€ ëŒ€ê¸°
        except Exception as e:
            print(f"âš ï¸ ì´ˆê¸° ì¿ í‚¤ í™•ë³´ ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ í¬ë¡¤ë§ì€ ê³„ì† ì‹œë„

        # 1. HTML ì†ŒìŠ¤ ì½”ë“œ ê°€ì ¸ì˜¤ê¸° (ì„¸ì…˜ ì‚¬ìš©)
        try:
            # í¬ë¡¤ë§ ë§¤ë„ˆë¥¼ ìœ„í•´ ìš”ì²­ ì „ì— ì ì‹œ ëŒ€ê¸°
            time.sleep(1.5) # ì‹œê°„ì„ 1.5ì´ˆë¡œ ë” ëŠ˜ë ¤ ë´‡ ê°ì§€ íšŒí”¼ ì‹œë„
            
            # session ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì²­, íƒ€ì„ì•„ì›ƒ 10ì´ˆ ì ìš©
            response = self.session.get(url, headers=self.HEADERS, timeout=10)
            response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ ë°œìƒ
        except requests.exceptions.HTTPError as e:
            # 403 ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ ì¶œë ¥
            print(f"âŒ HTTP ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

        # 2. BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        comments_data = []
        
        # ê²Œì‹œê¸€ ì‘ì„±ì ì¶”ì¶œ
        self.post_author = self._extract_post_author(soup)
        if self.post_author:
            print(f"ê²Œì‹œê¸€ ì‘ì„±ì: {self.post_author}")
            
        # 3. ëŒ“ê¸€ ìš”ì†Œ ì°¾ê¸° ë° ë°ì´í„° ì¶”ì¶œ
        comment_elements = soup.select('.comment-item')

        if not comment_elements:
            # ì½˜í…ì¸ ê°€ ì—†ëŠ” ê²½ìš°, ì„œë²„ê°€ ë¹„ì •ìƒì ì¸ ì‘ë‹µì„ ë³´ëƒˆì„ ê°€ëŠ¥ì„±ë„ í™•ì¸
            if response.status_code == 200 and 'ëŒ“ê¸€' not in response.text:
                 print("âš ï¸ HTML ë‚´ìš©ì— ëŒ“ê¸€ ì˜ì—­ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì°¨ë‹¨/ë³´ì•ˆ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("âš ï¸ ëŒ“ê¸€ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"ğŸ” ì´ {len(comment_elements)}ê°œì˜ ëŒ“ê¸€ ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        for comment in comment_elements:
            try:
                # ë‹‰ë„¤ì„ ì¶”ì¶œ
                user_info = comment.select_one('.user-info a')
                author = ""
                if not user_info:
                    # ìµëª… ë˜ëŠ” íƒˆí‡´ì
                    author_span = comment.select_one('.user-info span.name')
                    author = author_span.get_text(strip=True) if author_span else "ìµëª…/ì •ë³´ ì—†ìŒ"
                else:
                    # data-filter ë˜ëŠ” í…ìŠ¤íŠ¸ ë‚´ìš© ì‚¬ìš©
                    author = user_info.get('data-filter', '').strip() or user_info.get_text(strip=True)
                
                # ê²Œì‹œê¸€ ì‘ì„±ì ì œì™¸
                if self.post_author and author == self.post_author:
                    continue

                # ëŒ“ê¸€ ë‚´ìš© ì¶”ì¶œ
                message_elem = comment.select_one('.message')
                content = message_elem.get_text(strip=True) if message_elem else ""
                
                # ì´ëª¨í‹°ì½˜ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
                if not content and comment.select_one('.emoticon-wrapper'):
                    content = "[ì´ëª¨í‹°ì½˜]"

                # ì‘ì„± ì‹œê°„ ì¶”ì¶œ
                time_elem = comment.select_one('time')
                comment_time = time_elem.get_text(strip=True) if time_elem else "ì‹œê°„ ì •ë³´ ì—†ìŒ"

                if author and author not in ["ìµëª…/ì •ë³´ ì—†ìŒ", "ì‘ì„±ì"]:
                    comments_data.append({
                        'author': author,
                        'content': content if content else "[ë‚´ìš© ì—†ìŒ]",
                        'time': comment_time
                    })
                    
            except Exception as e:
                # íŠ¹ì • ëŒ“ê¸€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                print(f"âš ï¸ íŠ¹ì • ëŒ“ê¸€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
                
        return comments_data
